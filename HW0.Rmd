---
title: "CS6190: Probabilistic Modeling Homework 0"
author: "Gopal Menon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
   - \usepackage{mathtools}
   - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\begin{enumerate}
\item Let $X$ and $Y$ be continuous real-valued random variables. Prove the following:
\begin{enumerate}
\item $E\left[ E\left[X|Y \right]\right] = E\left[ X\right]$

$$
\begin{aligned}
E\left[ E\left[X|Y \right]\right] &= \int_{-\infty}^{\infty}E\left[X|Y=y \right]p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)dx\;p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)p_Y(y)dxdy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{XY}(x, y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}p_{XY}(x, y)dy\;dx\\
&= \int_{-\infty}^{\infty}x\;p_X(x)dx\\
&= E\left[ X\right]
\end{aligned}
$$
Here $p_X(x)$, $p_Y(y)$ and $p_{X|Y=y}(x)$ are the probability density functions for $X$, $Y$, and the conditional probability of $X$ given the value of $Y$. $p_{XY}(x,y)$ is the joint probability distribution function of $X$ and $Y$. I needed to look up the proof \cite{conexp} as I was not aware that the first step shown above could be done and due to this, I was getting an extra $y$ in the final result. The result I got before I looked up the proof was $yE\left[ X \right]$.

\item $Var(X) = E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right])$

$$
\begin{aligned}
E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right]) &= E\left[ E\left[X^2|Y\right] - E\left[X|Y\right] E\left[X|Y\right] \right] \\ &+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[ E\left[ X|Y \right]\right]E\left[ E\left[ X|Y \right]\right]\\
&= E \left[ E \left[ X^2|Y\right] \right] - E\left[ E\left[X|Y\right] E\left[X|Y\right] \right]\\&+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[X\right]E\left[X\right]\\
&= E\left[X^2 \right] - E\left[X\right]E\left[X\right]\\
&= Var(X)
\end{aligned}
$$

Before I worked out the solution above, I was using the definition $Var(X) = E\left[X^2\right] - \mu_X^2$ and that did not help. After I realized that $\mu_X = E\left[X\right]$, the solution fell in place.
\end{enumerate}

\item Consider random variables $X$ and $Y$ with joint pdf
$$
    p(x,y)= 
\begin{dcases}
    x+y,& \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,              & \text{otherwise}
\end{dcases}
$$
\begin{enumerate}

\item What is the marginal pdf $p(x)$?
$$
\begin{aligned}
p(x) &=  \int_{-\infty}^{\infty}p(x,y)dy\\
&=\begin{dcases}
    \int_0^1(x+y)dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(xy +\frac{1}{2}y^2 \right)\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    x+\frac{1}{2}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\item What is the conditional pdf $p(y|x)$?

$$
\begin{aligned}
p(y|x) &= \frac{p(x,y)}{p(x)}\\
 &=\begin{dcases}
    \frac{x+y}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$

\item What is the conditional expectation $E\left[Y|X\right]$?
$$
\begin{aligned}
E\left[Y|X\right] &= \int_{-\infty}^{\infty}p_{Y|X}(y)dy\\
 &=\begin{dcases}
    \int_{0}^{1}\frac{x+y}{x+\frac{1}{2}}dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{xy+\frac{1}{2}y^2}{x+\frac{1}{2}}\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{x+\frac{1}{2}}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$
\item What is the covariance $Cov(X,Y)$?

$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\\\
E\left[XY\right] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyp(x,y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}yp(x,y)dy &= \begin{dcases}
      \int_0^1 y(x+y)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 (xy+y^2)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}xy^2+\frac{1}{3}y^3\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}x+\frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx &= \begin{dcases}
      \int_0^1 x\left(\frac{1}{2}x+\frac{1}{3}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 \left(\frac{1}{2}x^2+\frac{1}{3}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{2}\frac{1}{3}x^3+\frac{1}{3}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{6}+\frac{1}{6}\right), \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= E\left[XY\right]\\
\end{aligned}
$$
$$
\begin{aligned}
E\left[X\right] &= \int_{-\infty}^{\infty}xp(x)dx\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}x \left(x+\frac{1}{2}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(x^2+\frac{1}{2}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}x^3 +\frac{1}{2}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
p(y) &= \int_{-\infty}^{\infty}p(x,y)dx\\
&=\begin{dcases}
    \int_0^1(x+y)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(\frac{1}{2}x^2 + yx\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    y+\frac{1}{2}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\\end{aligned}
$$

$$
\begin{aligned}
E\left[Y\right] &= \int_{-\infty}^{\infty}yp(y)dy\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}y \left(y+\frac{1}{2}\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(y^2+\frac{1}{2}y\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}y^3 +\frac{1}{2}\frac{1}{2}y^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}\\
$$
$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\
&= \begin{dcases}
      \frac{1}{3} - \frac{7}{12}\frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{48}{144} - \frac{49}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
     -\frac{1}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\end{enumerate}

\end{enumerate}

\begin{thebibliography}{9}
 
\bibitem{conexp} 
\textit{Conditional Expectation}, www.statlect.com/fundamentals-of-probability/conditional-expectation.
 
\end{thebibliography}
