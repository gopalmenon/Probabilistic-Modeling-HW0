---
title: 'CS6190: Probabilistic Modeling Homework 0'
author: "Gopal Menon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\begin{enumerate}
\item Let $X$ and $Y$ be continuous real-valued random variables. Prove the following:
\begin{enumerate}
\item $E\left[ E\left[X|Y \right]\right] = E\left[ X\right]$

$$
\begin{aligned}
E\left[ E\left[X|Y \right]\right] &= \int_{-\infty}^{\infty}E\left[X|Y=y \right]p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)dx\;p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)p_Y(y)dxdy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{XY}(x, y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}p_{XY}(x, y)dy\;dx\\
&= \int_{-\infty}^{\infty}x\;p_X(x)dx\\
&= E\left[ X\right]
\end{aligned}
$$
Here $p_X(x)$, $p_Y(y)$ and $p_{X|Y=y}(x)$ are the probability density functions for $X$, $Y$, and the conditional probability of $X$ given the value of $Y$. $p_{XY}(x,y)$ is the joint probability distribution function of $X$ and $Y$. I needed to look up the proof \cite{conexp} as I was not aware that the first step shown above could be done and due to this, I was getting an extra $y$ in the final result. The result I got before I looked up the proof was $yE\left[ X \right]$.

\item $Var(X) = E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right])$

$$
\begin{aligned}
E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right]) &= E\left[ E\left[X^2|Y\right] - E\left[X|Y\right] E\left[X|Y\right] \right] \\ &+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[ E\left[ X|Y \right]\right]E\left[ E\left[ X|Y \right]\right]\\
&= E \left[ E \left[ X^2|Y\right] \right] - E\left[ E\left[X|Y\right] E\left[X|Y\right] \right]\\&+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[X\right]E\left[X\right]\\
&= E\left[X^2 \right] - E\left[X\right]E\left[X\right]\\
&= Var(X)
\end{aligned}
$$

Before I worked out the solution above, I was using the definition $Var(X) = E\left[X^2\right] - \mu_X^2$ and that did not help. After I realized that $\mu_X = E\left[X\right]$, the solution fell in place.
\end{enumerate}

\item Consider random variables $X$ and $Y$ with joint pdf
$$
    p(x,y)= 
\begin{dcases}
    x+y,& \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,              & \text{otherwise}
\end{dcases}
$$
\begin{enumerate}

\item What is the marginal pdf $p(x)$?
$$
\begin{aligned}
p(x) &=  \int_{-\infty}^{\infty}p(x,y)dy\\
&=\begin{dcases}
    \int_0^1(x+y)dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(xy +\frac{1}{2}y^2 \right)\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    x+\frac{1}{2}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\item What is the conditional pdf $p(y|x)$?

$$
\begin{aligned}
p(y|x) &= \frac{p(x,y)}{p(x)}\\
 &=\begin{dcases}
    \frac{x+y}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$

\item What is the conditional expectation $E\left[Y|X\right]$?
$$
\begin{aligned}
E\left[Y|X\right] &= \int_{-\infty}^{\infty}p_{Y|X}(y)dy\\
 &=\begin{dcases}
    \int_{0}^{1}\frac{x+y}{x+\frac{1}{2}}dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{xy+\frac{1}{2}y^2}{x+\frac{1}{2}}\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{x+\frac{1}{2}}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$
\item What is the covariance $Cov(X,Y)$?

$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\\\
E\left[XY\right] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyp(x,y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}yp(x,y)dy &= \begin{dcases}
      \int_0^1 y(x+y)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 (xy+y^2)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}xy^2+\frac{1}{3}y^3\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}x+\frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx &= \begin{dcases}
      \int_0^1 x\left(\frac{1}{2}x+\frac{1}{3}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 \left(\frac{1}{2}x^2+\frac{1}{3}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{2}\frac{1}{3}x^3+\frac{1}{3}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{6}+\frac{1}{6}\right), \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= E\left[XY\right]\\
\end{aligned}
$$
$$
\begin{aligned}
E\left[X\right] &= \int_{-\infty}^{\infty}xp(x)dx\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}x \left(x+\frac{1}{2}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(x^2+\frac{1}{2}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}x^3 +\frac{1}{2}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
p(y) &= \int_{-\infty}^{\infty}p(x,y)dx\\
&=\begin{dcases}
    \int_0^1(x+y)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(\frac{1}{2}x^2 + yx\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    y+\frac{1}{2}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\\end{aligned}
$$

$$
\begin{aligned}
E\left[Y\right] &= \int_{-\infty}^{\infty}yp(y)dy\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}y \left(y+\frac{1}{2}\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(y^2+\frac{1}{2}y\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}y^3 +\frac{1}{2}\frac{1}{2}y^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}\\
$$
$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\
&= \begin{dcases}
      \frac{1}{3} - \frac{7}{12}\frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{48}{144} - \frac{49}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
     -\frac{1}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\end{enumerate}

\item Let $X \sim Exp(\lambda)$, i.e., the exponential distribution with pdf $p(x) = \lambda exp(-\lambda x)$. Let $Y = \sqrt{X}$.

\begin{enumerate}

\item What is the density function $p(y)$?

$$
\begin{aligned}
Y&=\sqrt{X}\\
f(x) &= \sqrt{x}\\
f^{-1}(y) &= y^2\\
p(y) &= \left | \frac{d}{dy}(f^{-1}(y)) \right |p(f^{-1}(y))\\
&= \left | \frac{d}{dy}y^2 \right |p(y^2)\\
&= \begin{dcases}
     2y\lambda exp(-\lambda y^2), \text{for } y \geq 0\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\item What is the cdf, $F(y) = P(Y \leq y)$? Verify that $F(0) = 0$ and $F(\infty) = 1$.

The cdf can be found by integrating the pdf.
$$
\begin{aligned}
F(y) &= P(Y \leq y) = \int_0^y 2y \lambda exp(-\lambda y^2)dy\\
\text{Substituting } u &= -\lambda y^2 \text{, we get } dy = -\frac{1}{2 \lambda y}du\\
\int 2y \lambda exp(-\lambda y^2)dy &= \int 2y \lambda exp(u) \times -\frac{1}{2 \lambda y}du\\
&= -\int exp(u) du\\
&= -exp(u)\\
&= -exp(-\lambda y^2)\\
F(y) &= -exp(-\lambda y^2)\Big|_0^y\\
&=-exp(-\lambda y^2) - (-exp(-\lambda 0^2))\\
&=-exp(-\lambda y^2) + 1\\
&= 1-exp(-\lambda y^2)\\
F(0)&=1-exp(-\lambda 0^2)\\
&= 1-1\\
&=0\\
F(\infty)&= 1-exp(-\infty)\\
&=1-0\\
&= 1
\end{aligned}
$$
\item What is the quantile function $F^{-1}$?

If $q_p$ is the quantile value given by the quantile function $F^{-1}(q_p)$, based on the cdf above:

$$
\begin{aligned}
q_p &=1-exp(-\lambda {(F^{-1}(q_p))}^2)\\
exp(-\lambda {(F^{-1}(q_p))}^2) &= 1-q_p\\
-\lambda {(F^{-1}(q_p))}^2 &= \ln(1-q_p)\\
{(F^{-1}(q_p))}^2 &= \frac{-1}{\lambda}\ln(1-q_p)\\
F^{-1}(q_p) &= \sqrt{\frac{-1}{\lambda}\ln(1-q_p)}
\end{aligned}
$$

\item Compute the mean, $E[Y]$, and variance, $Var(Y)$. \textbf{Hint:} Use integration by parts.

\end{enumerate}

\end{enumerate}

\begin{thebibliography}{9}
 
\bibitem{conexp} 
\textit{Conditional Expectation}, www.statlect.com/fundamentals-of-probability/conditional-expectation.
 
\end{thebibliography}
