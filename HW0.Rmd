---
title: 'CS6190: Probabilistic Modeling Homework 0'
author: "Gopal Menon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{mathtools}
- \usepackage{amsmath}
- \DeclareMathOperator{\Unif}{Unif}
- \DeclareMathOperator{\E}{E}
- \DeclareMathOperator{\Var}{Var}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{enumerate}
\item Let $X$ and $Y$ be continuous real-valued random variables. Prove the following:
\begin{enumerate}
\item $E\left[ E\left[X|Y \right]\right] = E\left[ X\right]$

$$
\begin{aligned}
E\left[ E\left[X|Y \right]\right] &= \int_{-\infty}^{\infty}E\left[X|Y=y \right]p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)dx\;p_Y(y)dy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{X|Y=y}(x)p_Y(y)dxdy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x\;p_{XY}(x, y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}p_{XY}(x, y)dy\;dx\\
&= \int_{-\infty}^{\infty}x\;p_X(x)dx\\
&= E\left[ X\right]
\end{aligned}
$$
Here $p_X(x)$, $p_Y(y)$ and $p_{X|Y=y}(x)$ are the probability density functions for $X$, $Y$, and the conditional probability of $X$ given the value of $Y$. $p_{XY}(x,y)$ is the joint probability distribution function of $X$ and $Y$. I needed to look up the proof \cite{conexp} as I was not aware that the first step shown above could be done and due to this, I was getting an extra $y$ in the final result. The result I got before I looked up the proof was $yE\left[ X \right]$.

\item $Var(X) = E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right])$

$$
\begin{aligned}
E\left[Var(X|Y) \right] + Var(E\left[ X|Y\right]) &= E\left[ E\left[X^2|Y\right] - E\left[X|Y\right] E\left[X|Y\right] \right] \\ &+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[ E\left[ X|Y \right]\right]E\left[ E\left[ X|Y \right]\right]\\
&= E \left[ E \left[ X^2|Y\right] \right] - E\left[ E\left[X|Y\right] E\left[X|Y\right] \right]\\&+ E\left[ E\left[X|Y\right] E\left[X|Y\right]\right] - E\left[X\right]E\left[X\right]\\
&= E\left[X^2 \right] - E\left[X\right]E\left[X\right]\\
&= Var(X)
\end{aligned}
$$

Before I worked out the solution above, I was using the definition $Var(X) = E\left[X^2\right] - \mu_X^2$ and that did not help. After I realized that $\mu_X = E\left[X\right]$, the solution fell in place.
\end{enumerate}

\item Consider random variables $X$ and $Y$ with joint pdf
$$
    p(x,y)= 
\begin{dcases}
    x+y,& \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,              & \text{otherwise}
\end{dcases}
$$
\begin{enumerate}

\item What is the marginal pdf $p(x)$?
$$
\begin{aligned}
p(x) &=  \int_{-\infty}^{\infty}p(x,y)dy\\
&=\begin{dcases}
    \int_0^1(x+y)dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(xy +\frac{1}{2}y^2 \right)\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    x+\frac{1}{2}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\item What is the conditional pdf $p(y|x)$?

$$
\begin{aligned}
p(y|x) &= \frac{p(x,y)}{p(x)}\\
 &=\begin{dcases}
    \frac{x+y}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$

\item What is the conditional expectation $E\left[Y|X\right]$?
$$
\begin{aligned}
E\left[Y|X\right] &= \int_{-\infty}^{\infty}p_{Y|X}(y)dy\\
 &=\begin{dcases}
    \int_{0}^{1}\frac{x+y}{x+\frac{1}{2}}dy, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{xy+\frac{1}{2}y^2}{x+\frac{1}{2}}\Big|_0^1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    \frac{x+\frac{1}{2}}{x+\frac{1}{2}}, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
 &=\begin{dcases}
    1, \text{for } 0\leq x \leq 1\\
    0,               \text{otherwise}
\end{dcases}
\end{aligned}
$$
\item What is the covariance $Cov(X,Y)$?

$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\\\
E\left[XY\right] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyp(x,y)dxdy\\
&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}yp(x,y)dy &= \begin{dcases}
      \int_0^1 y(x+y)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 (xy+y^2)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}xy^2+\frac{1}{3}y^3\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{2}x+\frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
\int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}yp(x,y)dydx &= \begin{dcases}
      \int_0^1 x\left(\frac{1}{2}x+\frac{1}{3}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_0^1 \left(\frac{1}{2}x^2+\frac{1}{3}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{2}\frac{1}{3}x^3+\frac{1}{3}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{6}+\frac{1}{6}\right), \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= E\left[XY\right]\\
\end{aligned}
$$
$$
\begin{aligned}
E\left[X\right] &= \int_{-\infty}^{\infty}xp(x)dx\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}x \left(x+\frac{1}{2}\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(x^2+\frac{1}{2}x\right)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}x^3 +\frac{1}{2}\frac{1}{2}x^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$
$$
\begin{aligned}
p(y) &= \int_{-\infty}^{\infty}p(x,y)dx\\
&=\begin{dcases}
    \int_0^1(x+y)dx, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
   \left(\frac{1}{2}x^2 + yx\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&=\begin{dcases}
    y+\frac{1}{2}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\\end{aligned}
$$

$$
\begin{aligned}
E\left[Y\right] &= \int_{-\infty}^{\infty}yp(y)dy\\
&= \begin{dcases}
      \int_{-\infty}^{\infty}y \left(y+\frac{1}{2}\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \int_{-\infty}^{\infty} \left(y^2+\frac{1}{2}y\right)dy, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \left(\frac{1}{3}y^3 +\frac{1}{2}\frac{1}{2}y^2\right)\Big|_0^1, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{1}{3} +\frac{1}{4}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}\\
$$
$$
\begin{aligned}
Cov(X,Y) &= E\left[XY\right] - E\left[X\right]E\left[Y\right]\\
&= \begin{dcases}
      \frac{1}{3} - \frac{7}{12}\frac{7}{12}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
      \frac{48}{144} - \frac{49}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
&= \begin{dcases}
     -\frac{1}{144}, \text{for } 0\leq x \leq 1, \text{and } 0\leq y \leq 1\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\end{enumerate}

\item Let $X \sim Exp(\lambda)$, i.e., the exponential distribution with pdf $p(x) = \lambda exp(-\lambda x)$. Let $Y = \sqrt{X}$.

\begin{enumerate}

\item What is the density function $p(y)$?

$$
\begin{aligned}
Y&=\sqrt{X}\\
f(x) &= \sqrt{x}\\
f^{-1}(y) &= y^2\\
p(y) &= \left | \frac{d}{dy}(f^{-1}(y)) \right |p(f^{-1}(y))\\
&= \left | \frac{d}{dy}y^2 \right |p(y^2)\\
&= \begin{dcases}
     2y\lambda exp(-\lambda y^2), \text{for } y \geq 0\\
    0,               \text{otherwise}
\end{dcases}\\
\end{aligned}
$$

\item What is the cdf, $F(y) = P(Y \leq y)$? Verify that $F(0) = 0$ and $F(\infty) = 1$.

The cdf can be found by integrating the pdf.
$$
\begin{aligned}
F(y) &= P(Y \leq y) = \int_0^y 2y \lambda exp(-\lambda y^2)dy\\
\text{Substituting } u &= -\lambda y^2 \text{, we get } dy = -\frac{1}{2 \lambda y}du\\
\int 2y \lambda exp(-\lambda y^2)dy &= \int 2y \lambda exp(u) \times -\frac{1}{2 \lambda y}du\\
&= -\int exp(u) du\\
&= -exp(u)\\
&= -exp(-\lambda y^2)\\
F(y) &= -exp(-\lambda y^2)\Big|_0^y\\
&=-exp(-\lambda y^2) - (-exp(-\lambda 0^2))\\
&=-exp(-\lambda y^2) + 1\\
&= 1-exp(-\lambda y^2)\\
F(0)&=1-exp(-\lambda 0^2)\\
&= 1-1\\
&=0\\
F(\infty)&= 1-exp(-\infty)\\
&=1-0\\
&= 1
\end{aligned}
$$
\item What is the quantile function $F^{-1}$?

If $q_p$ is the quantile value given by the quantile function $F^{-1}(q_p)$, based on the cdf above:

$$
\begin{aligned}
q_p &=1-exp(-\lambda {(F^{-1}(q_p))}^2)\\
exp(-\lambda {(F^{-1}(q_p))}^2) &= 1-q_p\\
-\lambda {(F^{-1}(q_p))}^2 &= \ln(1-q_p)\\
{(F^{-1}(q_p))}^2 &= \frac{-1}{\lambda}\ln(1-q_p)\\
F^{-1}(q_p) &= \sqrt{\frac{-1}{\lambda}\ln(1-q_p)}
\end{aligned}
$$

\item Compute the mean, $E[Y]$, and variance, $Var(Y)$. \textbf{Hint:} Use integration by parts.

$$
\begin{aligned}
E\left[Y\right] &= \int_{-\infty}^{\infty} y p(y) dy\\
&= \int_{0}^{\infty} 2y^2 \lambda exp(-\lambda y^2)dy = \int_{0}^{\infty} x^{\frac{1}{2}}\lambda exp(-\lambda x)\\
\end{aligned}\\
$$
$$
\begin{aligned}
Var(Y) &= E\left[Y^2\right] - E\left[Y\right]E\left[Y\right]\\
\end{aligned}
$$
\end{enumerate}

\item Given a realization $y_1, y_2, \ldots, y_n$ from your random variable $Y$
in the previous problem, what is the maximum likelihood estimate for $\lambda$?

The likelihood function will be
$$
\begin{aligned}
\mathcal{L}(\lambda) &= p_{Y_1, \ldots, Y_n}(y_1, y_2, \ldots, y_n; \lambda)\\
&= \prod_{i=1}^n p_{Y_i}(y_i; \lambda), 
  &\text{assuming $Y_i$ are independently distributed}\\
&= \prod_{i=1}^n \left(2y\lambda exp(-\lambda y_i^2) \right)\\
\ln(\mathcal{L}(\lambda)) = \ell(\lambda) &= \sum_{i=1}^n \ln\left(2y_i\lambda exp(-\lambda y_i^2) \right)\\
&= \sum_{i=1}^n \ln(2y_i) + \sum_{i=1}^n \ln(\lambda) - \sum_{i=1}^n \lambda y_i^2\\
\end{aligned}
$$
In order to find the maximum likelihood, we can find the maximum log likelihood since log is a monotonously increasing function. The first term above is a constant and we can find the maximum likelihood by setting the derivative with respect to $\lambda$ of the log likelihood to zero.
$$
\begin{aligned}
\frac{d \ell(\lambda)}{d\lambda} &= \frac{d \sum_{i=1}^n \ln(2y_i)}{d\lambda} + \frac{d \sum_{i=1}^n \ln(\lambda)}{d\lambda} - \frac{d \sum_{i=1}^n \lambda y_i^2}{d\lambda}\\
&= 0 + n\frac{d\ \ln(\lambda)}{d\lambda} - \sum_{i=1}^n y_i^2 \sum_{i=1}^n \frac{d\lambda}{d\lambda}\\
&= n\frac{1}{\lambda} - \sum_{i=1}^n y_i^2 = 0\\
\frac{n}{\lambda} &= \sum_{i=1}^ny_i^2\\
\lambda &= \frac{n}{\sum_{i=1}^ny_i^2}, 
  &\text{which is the maximum likely estimate for $\lambda$.}
\end{aligned}
$$

\end{enumerate}
\section*{R Coding Part}
\begin{enumerate}
\setcounter{enumi}{3}

\item Recall that if $F$ is a cdf and $U \sim \Unif(0,1)$, then $Y = F^{-1}(U)$
will be a random variable with cdf $F$. Write a function that uses this method
to generate $n$ random numbers from the distribution of $Y$ in Problem 3 above
($n$ and $\lambda$ should be parameters to the function).\\ {\bf Hint:} Look at
the {\tt runif} function in R for simulating uniform random varibles.

\begin{enumerate}
\item Generate 10,000 realizations of the random variable $Y$ with $\lambda = 2$.


\item Plot a histogram of these numbers, using the {\tt hist} function with
option {\tt freq = FALSE}.

\item Use the {\tt lines} command to plot the pdf of $Y$ on top.
\item Compute the sample mean and variance of your 10,000 realizations. Do they
roughly match the values for $\E[Y]$ and $\Var(Y)$ you calculated above?
\end{enumerate}

\item Now generate 20 realizations from $Y$ with $\lambda = 2$. Plot the log
likelihood function for this data. Plot a vertical line at the maximum
likelihood estimate for lambda (using the equation you got in Problem 3).

\end{enumerate}
\begin{thebibliography}{9}
 
\bibitem{conexp} 
\textit{Conditional Expectation}, www.statlect.com/fundamentals-of-probability/conditional-expectation.
 
\end{thebibliography}
